# Information-Theoretic XAI

This repository compiles publications on information-theoretic methods that could be used for Explainable Artificial Intelligence (XAI)

## Variational Information Bottleneck
1. Dai, Bin, Chen Zhu, and David Wipf. "Compressing neural networks using the variational information bottleneck." Proceedings of the 35th International Conference on Machine Learning (**ICML 2018**) [[PDF]](http://proceedings.mlr.press/v80/dai18d/dai18d.pdf)


## Mutual Information Maximization
1. Kong, Lingpeng, Cyprien de Masson d'Autume, Wang Ling, Lei Yu, Zihang Dai, and Dani Yogatama. "A Mutual Information Maximization Perspective of Language Representation Learning." (**ICLR 2020**) [[PDF]](https://openreview.net/pdf?id=Syx79eBKwr)
2. Song, Jiaming, and Stefano Ermon. "Understanding the Limitations of Variational Mutual Information Estimators." (**ICLR 2020**) [[PDF]](https://arxiv.org/pdf/1910.06222.pdf)
3. Poole, Ben, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. "On Variational Bounds of Mutual Information." In the 36th International Conference on Machine Learning, pp. 5171-5180. (**ICML 2019**) [[PDF]](http://proceedings.mlr.press/v97/poole19a/poole19a.pdf)
4. Tschannen, Michael, Josip Djolonga, Paul K. Rubenstein, Sylvain Gelly, and Mario Lucic. "On mutual information maximization for representation learning." arXiv preprint arXiv:1907.13625 (**2019**) [[PDF]](https://arxiv.org/pdf/1907.13625.pdf)
