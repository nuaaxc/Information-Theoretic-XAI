# Information-Theoretic XAI

This repository compiles publications on information-theoretic methods which **could be** used for Explainable Artificial Intelligence (XAI).

## Variational Information Bottleneck
1. Kolchinsky, Artemy, Brendan D. Tracey, and Steven Van Kuyk. "*Caveats for information bottleneck in deterministic scenarios*." (**ICLR 2019**) [[PDF]](https://openreview.net/pdf?id=rke4HiAcY7)
2. Saxe, Andrew M., Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky, Brendan D. Tracey, and David D. Cox. "*On the information bottleneck theory of deep learning*." (**ICLR 2019**) [[PDF]](https://openreview.net/pdf?id=rke4HiAcY7)
3. Bang, Seojin, Pengtao Xie, Wei Wu, and Eric Xing. "*Explaining a black-box using Deep Variational Information Bottleneck Approach*." arXiv preprint arXiv:1902.06918 (**2019**) [[PDF]](https://arxiv.org/pdf/1902.06918.pdf)
4. Li, Xiang Lisa, and Jason Eisner. "*Specializing Word Embeddings (for Parsing) by Information Bottleneck*." (**EMNLP 2019**) [[PDF]](https://arxiv.org/pdf/1910.00163.pdf)
5. Dai, Bin, Chen Zhu, and David Wipf. "*Compressing neural networks using the variational information bottleneck*." Proceedings of the 35th International Conference on Machine Learning (**ICML 2018**) [[PDF]](http://proceedings.mlr.press/v80/dai18d/dai18d.pdf)
6. Alemi, Alexander A., Ian Fischer, and Joshua V. Dillon. "*Uncertainty in the variational information bottleneck*." arXiv preprint arXiv:1807.00906 (**2018**) [[PDF]](https://arxiv.org/pdf/1807.00906.pdf)
7. Alemi, Alexander A., Ian Fischer, Joshua V. Dillon, and Kevin Murphy. "*Deep variational information bottleneck*." (**ICLR 2017**) [[PDF]](https://pdfs.semanticscholar.org/384c/808fc710468d874a89de4827f7b1a4367332.pdf)


## Mutual Information Maximization
1. Kong, Lingpeng, Cyprien de Masson d'Autume, Wang Ling, Lei Yu, Zihang Dai, and Dani Yogatama. "*A Mutual Information Maximization Perspective of Language Representation Learning*." (**ICLR 2020**) [[PDF]](https://openreview.net/pdf?id=Syx79eBKwr)
2. Song, Jiaming, and Stefano Ermon. "*Understanding the Limitations of Variational Mutual Information Estimators*." (**ICLR 2020**) [[PDF]](https://arxiv.org/pdf/1910.06222.pdf)
3. Poole, Ben, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. "*On Variational Bounds of Mutual Information*." In the 36th International Conference on Machine Learning, pp. 5171-5180. (**ICML 2019**) [[PDF]](http://proceedings.mlr.press/v97/poole19a/poole19a.pdf)
4. Tschannen, Michael, Josip Djolonga, Paul K. Rubenstein, Sylvain Gelly, and Mario Lucic. "*On mutual information maximization for representation learning*." arXiv preprint arXiv:1907.13625 (**2019**) [[PDF]](https://arxiv.org/pdf/1907.13625.pdf)
5. Gabrié, Marylou, Andre Manoel, Clément Luneau, Nicolas Macris, Florent Krzakala, and Lenka Zdeborová. "*Entropy and mutual information in models of deep neural networks*." In Advances in Neural Information Processing Systems, pp. 1821-1831. (**NIPS 2018**) [[PDF]](https://papers.nips.cc/paper/7453-entropy-and-mutual-information-in-models-of-deep-neural-networks.pdf)
